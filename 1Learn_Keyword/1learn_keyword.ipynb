{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords,words\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from autocorrect import spell\n",
    "import math\n",
    "from decimal import *\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords from differnt sources are copied into one text file and then read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Ritvik/Desktop/1Learn/final_stopwords.txt\"\n",
    "file = open(path, 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739\n"
     ]
    }
   ],
   "source": [
    "stop_words = wordpunct_tokenize(text)\n",
    "stop_words = set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the documents which we need for calculating idf..These text files are all added into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"CHAP 1.txt\",\"CHAP 2.txt\",\"CHAP 3.txt\",\"CHAP 4.txt\",\"CHAP 5.txt\",\"CHAP 6.txt\",\"CHAP 7.txt\",\"CHAP 8.txt\",\"CHAP 9.txt\",\"CHAP 10.txt\",\"CHAP 11.txt\",\"CHAP 12.txt\",\"CHAP 13.txt\",\"CHAP 14.txt\",\"CHAP 15.txt\",\"CHAP 16.txt\"]\n",
    "books = []\n",
    "path = \"C:/Users/Ritvik/Desktop/1Learn/Keyword Detector/ncert_text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    file = open(path+files[i], 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    books.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test file..tokenizing..removing stopwords..lemmatizing..also auto-correct.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2 = \"C:/Users/Ritvik/Desktop/1Learn/\"\n",
    "file = open(path_2 + \"test.txt\", 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449\n"
     ]
    }
   ],
   "source": [
    "words = wordpunct_tokenize(text)\n",
    "words = [w.lower() for w in words]\n",
    "print (len(words)) # lets see the no of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if w not in stop_words]\n",
    "words = [w for w in words if w.isalpha() == True and len(w) > 2]\n",
    "words = set(words)\n",
    "words = list(words)\n",
    "no_of_words = len(words)\n",
    "print (no_of_words) # Finally these number of words are used for keyword detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "words = [lmtzr.lemmatize(w) for w in words]\n",
    "count_of_words = len(words) # count is used when calculating term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shared', 'capable', 'pair', 'versatile', 'exceptionally', 'called', 'million', 'unique', 'molecule', 'specific', 'stable', 'formula', 'giving', 'double', 'larger', 'silicon', 'triple', 'strongly', 'valent', 'outnumbers', 'margin', 'upto', 'reactive', 'hold', 'structure', 'noticed', 'nitrogen', 'depend', 'saturated', 'form', 'compound', 'chemist', 'factor', 'ring', 'unsaturated', 'chlorine', 'strong', 'branched', 'size', 'property', 'element', 'linked', 'extent', 'mono', 'sharing', 'weaker', 'estimated', 'nature', 'element', 'chapter', 'formed', 'beginning', 'simple', 'atom', 'formation', 'single', 'addition', 'property', 'covalent', 'carbon', 'enables', 'oxygen', 'electron', 'exhibit', 'molecule', 'ability', 'compound', 'arranged', 'valency', 'sulphur', 'reason', 'recently', 'bond', 'hydrogen', 'bonding', 'rise', 'catenation', 'form', 'nucleus', 'chain', 'methane', 'bond']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shared', 'capable', 'pair', 'versatile', 'exceptionally', 'called', 'million', 'unique', 'molecule', 'specific', 'stable', 'formula', 'giving', 'double', 'larger', 'silicon', 'triple', 'strongly', 'valent', 'outnumbers', 'margin', 'unto', 'reactive', 'hold', 'structure', 'noticed', 'nitrogen', 'depend', 'saturated', 'form', 'compound', 'chemist', 'factor', 'ring', 'unsaturated', 'chlorine', 'strong', 'branched', 'size', 'property', 'element', 'linked', 'extent', 'mono', 'sharing', 'weaker', 'estimated', 'nature', 'element', 'chapter', 'formed', 'beginning', 'simple', 'atom', 'formation', 'single', 'addition', 'property', 'covalent', 'carbon', 'enables', 'oxygen', 'electron', 'exhibit', 'molecule', 'ability', 'compound', 'arranged', 'valency', 'sulphur', 'reason', 'recently', 'bond', 'hydrogen', 'bonding', 'rise', 'catenation', 'form', 'nucleus', 'chain', 'methane', 'bond']\n"
     ]
    }
   ],
   "source": [
    "words = [spell(w) for w in words]\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Frequency Distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary to store tf-idf value corresponding to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for i in words:\n",
    "    words_dict[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the list of doc we have 16 different text files.. tokenize each text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token={}  # Creating Dictionary 'token' to store tokenized word for each text file.\n",
    "for i in range(9):\n",
    "    token[i] = wordpunct_tokenize(books[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "no_of_docs = len(books) # used in idf formula\n",
    "print (no_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to calculate idf \n",
    "\n",
    "def IDF(key):\n",
    "    hit = 0 # to store no of doc having input word.\n",
    "    for i in token.keys():\n",
    "        for word in token[i]:\n",
    "            if word== key :\n",
    "                hit = hit + 1\n",
    "                break\n",
    "    (idf) = math.log10(Decimal(no_of_docs+1)/Decimal(hit+1)) \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating tf*idf for each word..\n",
    "\n",
    "for i in words:\n",
    "    getcontext().prec = 10\n",
    "    tf = Decimal(fd[i])/Decimal(count_of_words) # term frequency\n",
    "    idf = IDF(i) # Inverse Term Frequency\n",
    "    tf_idf = Decimal(tf)*Decimal(idf)\n",
    "    words_dict[i] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bond\n",
      "molecule\n",
      "unto\n",
      "exhibit\n",
      "versatile\n",
      "exceptionally\n",
      "triple\n",
      "valent\n",
      "outnumbers\n",
      "hold\n",
      "saturated\n",
      "chemist\n",
      "factor\n",
      "ring\n",
      "unsaturated\n",
      "branched\n",
      "mono\n",
      "sharing\n",
      "weaker\n",
      "enables\n",
      "bonding\n",
      "catenation\n",
      "chain\n",
      "compound\n",
      "property\n",
      "element\n",
      "shared\n",
      "capable\n",
      "million\n",
      "unique\n",
      "stable\n",
      "silicon\n",
      "margin\n",
      "estimated\n",
      "covalent\n",
      "valency\n",
      "recently\n",
      "methane\n",
      "double\n",
      "depend\n",
      "linked\n",
      "extent\n",
      "chapter\n",
      "atom\n",
      "electron\n",
      "formula\n",
      "strongly\n",
      "reactive\n",
      "nitrogen\n",
      "strong\n",
      "beginning\n",
      "addition\n",
      "ability\n",
      "arranged\n",
      "sulphur\n",
      "reason\n",
      "form\n",
      "pair\n",
      "specific\n",
      "larger\n",
      "noticed\n",
      "chlorine\n",
      "structure\n",
      "size\n",
      "formation\n",
      "single\n",
      "nucleus\n",
      "giving\n",
      "nature\n",
      "simple\n",
      "carbon\n",
      "hydrogen\n",
      "rise\n",
      "called\n",
      "formed\n",
      "oxygen\n"
     ]
    }
   ],
   "source": [
    "### Reverse Sorting dictionary according to tf-idf value..\n",
    "\n",
    "sorted_words_dict = sorted(words_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "for key, value in sorted_words_dict:\n",
    "    print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bond\n",
      "molecule\n",
      "unto\n",
      "exhibit\n",
      "versatile\n",
      "exceptionally\n",
      "triple\n"
     ]
    }
   ],
   "source": [
    "### Getting top 7 keywords\n",
    "\n",
    "count = 7\n",
    "for key,value in sorted_words_dict:\n",
    "    if len(key) > 3 and count != 0:\n",
    "        print (key)\n",
    "        count = count-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
