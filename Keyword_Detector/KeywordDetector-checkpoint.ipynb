{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords,words\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import math\n",
    "from decimal import *\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting stopwords from text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords from differnt sources are copied into one text file and then read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open(\"C:\\Users\\Swati\\Desktop\\stopwords.txt\",'r')\n",
    "f = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All stopwords are tokenized, i.e., converted into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734\n"
     ]
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(f)\n",
    "print len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English stopwords from nltk corpus are also added into the list ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2147\n",
      "765\n"
     ]
    }
   ],
   "source": [
    "stopwords_nltk = (stopwords.words('english'))\n",
    "stop_words = tokens + stopwords_nltk\n",
    "print len(stop_words)\n",
    "\n",
    "# Getting only unique words.\n",
    "stop_words = set(stop_words)\n",
    "print len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk's Book corpus is used as documents for the use of Inverse Document Frequency. All the nine text books are written into 'Book' folder by running Create_books.ipynb file. These text files are all added into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"Book1.txt\", \"Book2.txt\", \"Book3.txt\", \"Book4.txt\", \"Book5.txt\", \"Book6.txt\", \"Book7.txt\", \"Book8.txt\", \"Book9.txt\"]\n",
    "books = []\n",
    "path = \"C:/Users/Swati/Desktop/NLTK_Books/\"\n",
    "for i in range(len(files)):\n",
    "    file = open(path+files[i], 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    books.append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path + \"test.txt\", 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing text into words and converting into lower-case words for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "words = wordpunct_tokenize(text)\n",
    "words = [w.lower() for w in words]\n",
    "print len(words) # lets see the no of words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer function in used to lemmmatize input text as this will reduce all words into root words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "words = [lmtzr.lemmatize(w.decode('utf-8', \"replace\")) for w in words]\n",
    "count_of_words = len(words) # count is used when calculating term frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of stopwords from input text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if w not in stop_words]\n",
    "words = [w for w in words if w.isalpha() == True and len(w) > 1]\n",
    "words = set(words)\n",
    "words = list(words)\n",
    "no_of_words = len(words)\n",
    "print no_of_words # Finally these number of words are used for keyword detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Frequency Distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dictionary to store tf-idf value corresponding to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for i in words:\n",
    "    words_dict[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list 'books' we have 9 different text files from 'Books' corpora. Thus we have to first tokenize each text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token={}  # Creating Dictionary 'token' to store tokenized word for each text file.\n",
    "for i in range(9):\n",
    "    token[i] = wordpunct_tokenize(books[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "no_of_docs = len(books) # used in idf formula\n",
    "print no_of_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining IDF Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes 'key' as input word and finds the number of documents which contains this word and stores it into 'hit' variable. Finally IDF is calculated using smoothing formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(key):\n",
    "    hit = 0 # to store no of doc having input word.\n",
    "    for i in token.keys():\n",
    "        for word in token[i]:\n",
    "            if word== key :\n",
    "                hit = hit + 1\n",
    "                break\n",
    "    (idf) = math.log10(Decimal(no_of_docs+1)/Decimal(hit+1)) \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating tf-idf value for each word in 'words' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words:\n",
    "    getcontext().prec = 10\n",
    "    tf = Decimal(fd[i])/Decimal(count_of_words) # term frequency\n",
    "    idf = IDF(i) # Inverse Term Frequency\n",
    "    tf_idf = Decimal(tf)*Decimal(idf)\n",
    "    words_dict[i] = tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally reverse sorting words_dict to see which word has the the highest tf-idf value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'hacker', Decimal('0.02')), (u'neo', Decimal('0.02')), (u'metacortex', Decimal('0.02')), (u'morpheus', Decimal('0.02')), (u'terrorist', Decimal('0.01397940009')), (u'anderson', Decimal('0.01397940009')), (u'amnesty', Decimal('0.01397940009')), (u'engineer', Decimal('0.01045757490')), (u'virtually', Decimal('0.01045757490')), (u'software', Decimal('0.01045757490')), (u'smith', Decimal('0.007958800173')), (u'capture', Decimal('0.006020599913')), (u'agent', Decimal('0.006020599913')), (u'crime', Decimal('0.004436974994')), (u'dangerous', Decimal('0.004436974994')), (u'guilty', Decimal('0.003098039202')), (u'exchange', Decimal('0.003098039202')), (u'law', Decimal('0.001938200260')), (u'life', Decimal('0.00'))]\n"
     ]
    }
   ],
   "source": [
    "sorted_words_dict = sorted(words_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "print sorted_words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here constraints are added to find good keywords, i.e., only words having length greater than 3 will be used.\n",
    "\n",
    "Outputting only top 5 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacker\n",
      "metacortex\n",
      "morpheus\n",
      "terrorist\n",
      "anderson\n"
     ]
    }
   ],
   "source": [
    "count = 5\n",
    "for key,value in sorted_words_dict:\n",
    "    if len(key) > 3 and count != 0:\n",
    "        print key\n",
    "        count = count-1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "no_of_docs = len(books)\n",
    "print no_of_docss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
