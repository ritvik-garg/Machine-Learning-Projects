{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords,words\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from autocorrect import spell\n",
    "import math\n",
    "from decimal import *\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords from differnt sources are copied into one text file and then read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"C:/Users/Ritvik/Desktop/1Learn/final_stopwords.txt\"\n",
    "file = open(path, 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = wordpunct_tokenize(text)\n",
    "stop_words = set(stop_words)\n",
    "#print((stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the documents which we need for calculating idf..These text files are all added into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [\"CHAP 1.txt\",\"CHAP 2.txt\",\"CHAP 3.txt\",\"CHAP 4.txt\",\"CHAP 5.txt\",\"CHAP 6.txt\",\"CHAP 7.txt\",\"CHAP 8.txt\",\"CHAP 9.txt\",\"CHAP 10.txt\",\"CHAP 11.txt\",\"CHAP 12.txt\",\"CHAP 13.txt\",\"CHAP 14.txt\",\"CHAP 15.txt\",\"CHAP 16.txt\"]\n",
    "books = []\n",
    "path = \"C:/Users/Ritvik/Desktop/1Learn/Keyword Detector/ncert_text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    file = open(path+files[i], 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    books.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test file..tokenizing..removing stopwords..lemmatizing..also auto-correct.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_2 = \"C:/Users/Ritvik/Desktop/1Learn/\"\n",
    "file = open(path_2 + \"test.txt\", 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "words = wordpunct_tokenize(text)\n",
    "#print(words)\n",
    "words = [w.lower() for w in words]\n",
    "print (len(words)) # lets see the no of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))\n",
    "words = [w for w in words if w not in stop_words]\n",
    "words = [w for w in words if w.isalpha() == True and len(w) > 2]\n",
    "words = set(words)\n",
    "words = list(words)\n",
    "no_of_words = len(words)\n",
    "print (no_of_words) # Finally these number of words are used for keyword detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "words = [lmtzr.lemmatize(w) for w in words]\n",
    "count_of_words = len(words) # count is used when calculating term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['century', 'child', 'trait', 'paternal', 'contribution', 'mendel', 'human', 'main', 'ago', 'mother', 'dna', 'related', 'experiment', 'amount', 'genetic', 'trait', 'mean', 'box', 'influenced', 'inheritance', 'version', 'father', 'maternal', 'equal', 'contribute', 'practically', 'material', 'rule']\n"
     ]
    }
   ],
   "source": [
    "words = [spell(w) for w in words]\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Frequency Distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary to store tf-idf value corresponding to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for i in words:\n",
    "    words_dict[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the list of doc we have 16 different text files.. tokenize each text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token={}  # Creating Dictionary 'token' to store tokenized word for each text file.\n",
    "for i in range(9):\n",
    "    token[i] = wordpunct_tokenize(books[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "no_of_docs = len(books) # used in idf formula\n",
    "print (no_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to calculate idf \n",
    "\n",
    "def IDF(key):\n",
    "    hit = 0 # to store no of doc having input word.\n",
    "    for i in token.keys():\n",
    "        for word in token[i]:\n",
    "            if word== key :\n",
    "                hit = hit + 1\n",
    "                break\n",
    "    (idf) = math.log10(Decimal(no_of_docs+1)/Decimal(hit+1)) \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculating tf*idf for each word..\n",
    "\n",
    "for i in words:\n",
    "    getcontext().prec = 10\n",
    "    tf = Decimal(fd[i])/Decimal(count_of_words) # term frequency\n",
    "    idf = IDF(i) # Inverse Term Frequency\n",
    "    tf_idf = Decimal(tf)*Decimal(idf)\n",
    "    words_dict[i] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "[('trait', Decimal('0.06638706612')), ('mendel', Decimal('0.04394460433')), ('dna', Decimal('0.04394460433')), ('version', Decimal('0.04394460433')), ('century', Decimal('0.03319353306')), ('paternal', Decimal('0.03319353306')), ('contribution', Decimal('0.03319353306')), ('influenced', Decimal('0.03319353306')), ('father', Decimal('0.03319353306')), ('maternal', Decimal('0.03319353306')), ('contribute', Decimal('0.03319353306')), ('practically', Decimal('0.03319353306')), ('rule', Decimal('0.03319353306')), ('child', Decimal('0.02690455952')), ('ago', Decimal('0.02690455952')), ('genetic', Decimal('0.02690455952')), ('box', Decimal('0.02690455952')), ('inheritance', Decimal('0.02690455952')), ('mother', Decimal('0.02244246178')), ('human', Decimal('0.01898138989')), ('main', Decimal('0.01898138989')), ('related', Decimal('0.01898138989')), ('mean', Decimal('0.01898138989')), ('equal', Decimal('0.01898138989')), ('experiment', Decimal('0.01615348825')), ('material', Decimal('0.01169139051')), ('amount', Decimal('0.009864514712'))]\n"
     ]
    }
   ],
   "source": [
    "### Reverse Sorting dictionary according to tf-idf value..\n",
    "\n",
    "sorted_words_dict = sorted(words_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "print(len(sorted_words_dict))\n",
    "print (sorted_words_dict)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  This block compares text with the list having all topics and sub-topics.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Getting top 7 keywords\n",
    "#print (\"sure_keyword=\", sure_keyword)\n",
    "count=5\n",
    "if(sure_keyword == 0):\n",
    "    for key,value in sorted_words_dict:\n",
    "        if len(key) > 3 and count > 0:\n",
    "            print (key)\n",
    "            count = count-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
